"""
åŸºäº Gradio çš„ LangGraph æ™ºèƒ½ä½“ç½‘é¡µèŠå¤©ç•Œé¢
"""

from typing import TypedDict, Annotated, Optional
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
# from langchain_groq import ChatGroq  # æš‚æ—¶æ³¨é‡Šï¼Œéœ€è¦æ—¶å†å¯ç”¨
from dotenv import load_dotenv
import operator
import os
import gradio as gr

# åŠ è½½ç¯å¢ƒå˜é‡
env_loaded = load_dotenv()

# è¯»å–æ‰€æœ‰ API Keys
zhipu_api_key = os.getenv("ZHIPU_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")
groq_api_key = os.getenv("GROQ_API_KEY")
siliconflow_api_key = os.getenv("SILICONFLOW_API_KEY")

# å®šä¹‰å¯ç”¨çš„æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    # æ™ºè°± AI æ¨¡å‹ï¼ˆå…è´¹ï¼‰
    "æ™ºè°±-GLM-4-Flash": {
        "provider": "zhipu",
        "model_name": "glm-4-flash",
        "api_key": zhipu_api_key,
        "base_url": "https://open.bigmodel.cn/api/paas/v4/",
        "description": "å…è´¹ | å¿«é€Ÿå“åº” | æ™ºèƒ½å¯¹è¯"
    },
    "æ™ºè°±-GLM-4": {
        "provider": "zhipu",
        "model_name": "glm-4",
        "api_key": zhipu_api_key,
        "base_url": "https://open.bigmodel.cn/api/paas/v4/",
        "description": "é«˜çº§ç‰ˆæœ¬ | æ›´å¼ºèƒ½åŠ›"
    },
    # ç¡…åŸºæµåŠ¨æ¨¡å‹ï¼ˆå…è´¹ï¼Œæ¨èï¼‰
    "ç¡…åŸº-Qwen2.5-7B": {
        "provider": "siliconflow",
        "model_name": "Qwen/Qwen2.5-7B-Instruct",
        "api_key": siliconflow_api_key,
        "base_url": "https://api.siliconflow.cn/v1",
        "description": "å…è´¹ | é€šä¹‰åƒé—® | å¿«é€Ÿ"
    },
    "ç¡…åŸº-DeepSeek-V2.5": {
        "provider": "siliconflow",
        "model_name": "deepseek-ai/DeepSeek-V2.5",
        "api_key": siliconflow_api_key,
        "base_url": "https://api.siliconflow.cn/v1",
        "description": "å…è´¹ | DeepSeek | å¼ºæ¨ç†"
    },
    "ç¡…åŸº-GLM-4-9B": {
        "provider": "siliconflow",
        "model_name": "THUDM/glm-4-9b-chat",
        "api_key": siliconflow_api_key,
        "base_url": "https://api.siliconflow.cn/v1",
        "description": "å…è´¹ | æ™ºè°±GLM | å¤šåŠŸèƒ½"
    },
    # OpenAI æ¨¡å‹
    "OpenAI-GPT-3.5-Turbo": {
        "provider": "openai",
        "model_name": "gpt-3.5-turbo",
        "api_key": openai_api_key,
        "base_url": None,
        "description": "ä»˜è´¹ | ç»å…¸æ¨¡å‹ | ç¨³å®šå¯é "
    },
    "OpenAI-GPT-4": {
        "provider": "openai",
        "model_name": "gpt-4",
        "api_key": openai_api_key,
        "base_url": None,
        "description": "ä»˜è´¹ | æœ€å¼ºèƒ½åŠ› | å¤æ‚æ¨ç†"
    },
    "OpenAI-GPT-4-Turbo": {
        "provider": "openai",
        "model_name": "gpt-4-turbo-preview",
        "api_key": openai_api_key,
        "base_url": None,
        "description": "ä»˜è´¹ | æ›´å¿«çš„ GPT-4"
    },
}

# å½“å‰ä½¿ç”¨çš„æ¨¡å‹
current_llm = None
current_model_name = None


def create_llm(model_key: str):
    """æ ¹æ®æ¨¡å‹é”®åˆ›å»ºå¯¹åº”çš„ LLM å®ä¾‹"""
    if model_key not in AVAILABLE_MODELS:
        return None, f"æ¨¡å‹ {model_key} ä¸å­˜åœ¨"
    
    config = AVAILABLE_MODELS[model_key]
    
    # æ£€æŸ¥ API Key æ˜¯å¦é…ç½®
    if not config["api_key"]:
        provider_name = "æ™ºè°±AI" if config["provider"] == "zhipu" else config["provider"].upper()
        return None, f"âŒ æœªé…ç½® {provider_name} API Key"
    
    try:
        if config["provider"] in ["zhipu", "openai", "siliconflow"]:
            llm = ChatOpenAI(
                model=config["model_name"],
                temperature=0.7,
                api_key=config["api_key"],  # type: ignore
                base_url=config["base_url"]
            )
        elif config["provider"] == "groq":
            # Groq æš‚æ—¶ä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£… langchain-groq
            return None, "âš ï¸ Groq æ¨¡å‹éœ€è¦å®‰è£… langchain-groq åº“"
        else:
            return None, f"ä¸æ”¯æŒçš„æä¾›å•†: {config['provider']}"
        
        return llm, None
    except Exception as e:
        return None, f"åˆ›å»ºæ¨¡å‹å¤±è´¥: {str(e)}"


def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨ï¼ˆå·²é…ç½® API Keyï¼‰çš„æ¨¡å‹åˆ—è¡¨"""
    available = []
    for model_key, config in AVAILABLE_MODELS.items():
        if config["api_key"]:
            available.append(model_key)
    return available


def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    status = "### ç³»ç»ŸçŠ¶æ€\n\n"
    
    if env_loaded:
        status += "âœ… .env æ–‡ä»¶å·²åŠ è½½\n\n"
    else:
        status += "âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶\n\n"
    
    status += "### API é…ç½®\n\n"
    
    if zhipu_api_key:
        masked = zhipu_api_key[:8] + "***" + zhipu_api_key[-4:] if len(zhipu_api_key) > 12 else "***"
        status += f"- æ™ºè°± AI: {masked}\n"
    else:
        status += "- æ™ºè°± AI: âŒ æœªé…ç½®\n"
    
    if openai_api_key:
        masked = openai_api_key[:8] + "***" + openai_api_key[-4:] if len(openai_api_key) > 12 else "***"
        status += f"- OpenAI: {masked}\n"
    else:
        status += "- OpenAI: âŒ æœªé…ç½®\n"
    
    if groq_api_key:
        masked = groq_api_key[:8] + "***" + groq_api_key[-4:] if len(groq_api_key) > 12 else "***"
        status += f"- Groq: {masked}\n"
    else:
        status += "- Groq: âŒ æœªé…ç½®\n"
    
    if siliconflow_api_key:
        masked = siliconflow_api_key[:8] + "***" + siliconflow_api_key[-4:] if len(siliconflow_api_key) > 12 else "***"
        status += f"- ç¡…åŸºæµåŠ¨: {masked}\n"
    else:
        status += "- ç¡…åŸºæµåŠ¨: âŒ æœªé…ç½®\n"
    
    if current_model_name:
        status += f"\n### å½“å‰æ¨¡å‹\n\nğŸ“Œ {current_model_name}"
    else:
        status += "\n### å½“å‰æ¨¡å‹\n\nâš ï¸ æœªé€‰æ‹©æ¨¡å‹"
    
    return status


def update_status(selected_model):
    """æ›´æ–°çŠ¶æ€æ˜¾ç¤ºï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ‡æ¢éªŒè¯"""
    global current_model_name
    
    if selected_model != current_model_name:
        # å°è¯•åˆ‡æ¢æ¨¡å‹
        llm, error = create_llm(selected_model)
        if error:
            # åˆ‡æ¢å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ä½†ä¸æ›´æ–° current_model_name
            status = get_system_status()
            status += f"\n\n### âš ï¸ æ¨¡å‹åˆ‡æ¢å¤±è´¥\n\n{error}"
            return status
        else:
            # åˆ‡æ¢æˆåŠŸ
            current_model_name = selected_model
            status = get_system_status()
            status += "\n\n### âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ"
            return status
    else:
        return get_system_status()


def get_model_info():
    """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯ç”¨äºæ˜¾ç¤º"""
    info = "### ğŸ“‹ å¯ç”¨æ¨¡å‹\n\n"
    
    has_zhipu = bool(zhipu_api_key)
    has_openai = bool(openai_api_key)
    has_siliconflow = bool(siliconflow_api_key)
    
    if has_siliconflow:
        info += "**ç¡…åŸºæµåŠ¨** âœ… (å¼ºçƒˆæ¨è)\n"
        info += "- Qwen 2.5 7B (å…è´¹)\n"
        info += "- DeepSeek V2.5 (å…è´¹)\n"
        info += "- GLM-4 9B (å…è´¹)\n\n"
    else:
        info += "**ç¡…åŸºæµåŠ¨** âŒ æœªé…ç½®\n\n"
    
    if has_zhipu:
        info += "**æ™ºè°± AI** âœ…\n"
        info += "- GLM-4-Flash (å…è´¹æ¨è)\n"
        info += "- GLM-4 (é«˜çº§ç‰ˆæœ¬)\n\n"
    else:
        info += "**æ™ºè°± AI** âŒ æœªé…ç½®\n\n"
    
    if has_openai:
        info += "**OpenAI** âœ…\n"
        info += "- GPT-3.5-Turbo\n"
        info += "- GPT-4\n"
        info += "- GPT-4-Turbo\n\n"
    else:
        info += "**OpenAI** âŒ æœªé…ç½®\n\n"
    
    if not (has_zhipu or has_openai or has_siliconflow):
        info += "\nâš ï¸ è¯·è‡³å°‘é…ç½®ä¸€ä¸ª API Key\n"
    
    return info


# å®šä¹‰çŠ¶æ€ç»“æ„
class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    next_step: str
    result: str


# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def process_input(state: AgentState) -> AgentState:
    """å¤„ç†åˆå§‹è¾“å…¥"""
    messages = state.get("messages", [])
    
    if messages:
        last_message = messages[-1]
        
        # ç®€å•çš„å†³ç­–é€»è¾‘
        if "hello" in last_message.lower() or "ä½ å¥½" in last_message.lower() or "å“ˆå–½" in last_message.lower():
            state["next_step"] = "greet"
        elif "help" in last_message.lower() or "å¸®åŠ©" in last_message.lower():
            state["next_step"] = "provide_help"
        else:
            state["next_step"] = "general_response"
    
    return state


def greet_user(state: AgentState) -> AgentState:
    """å‘ç”¨æˆ·é—®å€™"""
    state["result"] = "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ LangGraph æ™ºèƒ½ä½“ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    state["next_step"] = "end"
    return state


def provide_help(state: AgentState) -> AgentState:
    """æä¾›å¸®åŠ©ä¿¡æ¯"""
    state["result"] = """
æˆ‘æ˜¯ä¸€ä¸ªç®€å•çš„ LangGraph æ™ºèƒ½ä½“ã€‚ä»¥ä¸‹æ˜¯æˆ‘èƒ½åšçš„äº‹æƒ…ï¼š
- å½“ä½ å‘æˆ‘é—®å€™æ—¶ï¼Œæˆ‘ä¼šæ‰“æ‹›å‘¼
- å½“ä½ éœ€è¦å¸®åŠ©æ—¶ï¼Œæˆ‘ä¼šæä¾›å¸®åŠ©ä¿¡æ¯
- å¯¹å…¶ä»–è¾“å…¥ï¼Œæˆ‘ä¼šä½¿ç”¨å¤§æ¨¡å‹ç»™å‡ºå›å¤

è¯•ç€è¯´ï¼š'ä½ å¥½'ã€'å¸®åŠ©' æˆ–å…¶ä»–ä»»ä½•å†…å®¹ï¼
    """
    state["next_step"] = "end"
    return state


def general_response(state: AgentState) -> AgentState:
    """æä¾›ä¸€èˆ¬æ€§å›å¤ï¼ˆä½¿ç”¨å¤§æ¨¡å‹ï¼‰"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else "æ— å†…å®¹"
    
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†å¤§æ¨¡å‹
    if current_llm is None:
        state["result"] = f"æ”¶åˆ°æ¶ˆæ¯ï¼š'{last_message}'\n\nâš ï¸ æœªé€‰æ‹©æˆ–é…ç½®æ¨¡å‹ã€‚\nè¯·åœ¨å³ä¾§é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹ã€‚"
        state["next_step"] = "end"
        return state
    
    try:
        # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
        response = current_llm.invoke(f"è¯·ç”¨å‹å¥½ã€ç®€æ´çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š{last_message}")
        result_content = str(response.content) if response.content else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        state["result"] = result_content
    except Exception as e:
        error_msg = str(e)
        if "Connection error" in error_msg or "è¿æ¥" in error_msg:
            state["result"] = f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥\n\nå¯èƒ½çš„åŸå› ï¼š\n1. ç½‘ç»œè¿æ¥é—®é¢˜\n2. API æœåŠ¡ä¸å¯ç”¨\n3. éœ€è¦ä»£ç†è®¿é—®\n\nè¯¦ç»†é”™è¯¯ï¼š{error_msg}"
        elif "API key" in error_msg or "401" in error_msg:
            state["result"] = f"âŒ API Key é”™è¯¯\n\nè¯·æ£€æŸ¥ï¼š\n1. API Key æ˜¯å¦æ­£ç¡®\n2. API Key æ˜¯å¦æœ‰æ•ˆ\n3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„é…é¢\n\nè¯¦ç»†é”™è¯¯ï¼š{error_msg}"
        else:
            state["result"] = f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†ä½ çš„æ¶ˆæ¯æ—¶é‡åˆ°äº†é—®é¢˜ï¼š{error_msg}"
    
    state["next_step"] = "end"
    return state


def route_next(state: AgentState) -> str:
    """å†³å®šä¸‹ä¸€ä¸ªè¦è®¿é—®çš„èŠ‚ç‚¹"""
    next_step = state.get("next_step", "end")
    
    if next_step == "greet":
        return "greet"
    elif next_step == "provide_help":
        return "help"
    elif next_step == "general_response":
        return "general"
    else:
        return "end"


# æ„å»ºå›¾
def create_agent_graph():
    """åˆ›å»ºå¹¶ç¼–è¯‘æ™ºèƒ½ä½“å›¾"""
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("process", process_input)
    workflow.add_node("greet", greet_user)
    workflow.add_node("help", provide_help)
    workflow.add_node("general", general_response)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("process")
    
    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "process",
        route_next,
        {
            "greet": "greet",
            "help": "help",
            "general": "general",
            "end": END
        }
    )
    
    # æ·»åŠ åˆ°ç»“æŸèŠ‚ç‚¹çš„è¾¹
    workflow.add_edge("greet", END)
    workflow.add_edge("help", END)
    workflow.add_edge("general", END)
    
    # ç¼–è¯‘å›¾
    app = workflow.compile()
    return app


# åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
agent = create_agent_graph()


def chat_response(message, history, selected_model):
    """å¤„ç†èŠå¤©æ¶ˆæ¯"""
    global current_llm, current_model_name
    
    # å¦‚æœæ¨¡å‹åˆ‡æ¢äº†ï¼Œé‡æ–°åˆ›å»º LLM
    if selected_model != current_model_name:
        print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ¢æ¨¡å‹: {current_model_name} -> {selected_model}")
        llm, error = create_llm(selected_model)
        if error:
            error_msg = f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥\n\n{error}\n\nè¯·æ£€æŸ¥ï¼š\n1. API Key æ˜¯å¦æ­£ç¡®é…ç½®\n2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n3. é€‰æ‹©å…¶ä»–å¯ç”¨æ¨¡å‹"
            print(f"âŒ åˆ‡æ¢å¤±è´¥: {error}")
            return error_msg
        current_llm = llm
        current_model_name = selected_model
        print(f"âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ: {selected_model}")
    
    try:
        # è¿è¡Œæ™ºèƒ½ä½“
        agent = create_agent_graph()
        result = agent.invoke({
            "messages": [message],
            "next_step": "",
            "result": ""
        })
        
        return result['result']
    except Exception as e:
        error_msg = f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™ï¼š{str(e)}\n\nå¯èƒ½çš„åŸå› ï¼š\n1. æ¨¡å‹ API è°ƒç”¨å¤±è´¥\n2. ç½‘ç»œè¿æ¥é—®é¢˜\n3. API é…é¢ä¸è¶³"
        print(f"âŒ é”™è¯¯: {str(e)}")
        return error_msg


def test_model_connection(selected_model):
    """æµ‹è¯•æ¨¡å‹è¿æ¥æ˜¯å¦æ­£å¸¸"""
    if not selected_model:
        return "âš ï¸ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹"
    
    print(f"\nğŸ” æ­£åœ¨æµ‹è¯•æ¨¡å‹: {selected_model}")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    llm, error = create_llm(selected_model)
    if error:
        result = f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥\n\n{error}\n\nè¯·æ£€æŸ¥ API Key é…ç½®"
        print(f"âŒ æµ‹è¯•å¤±è´¥: {error}")
        return result
    
    # å‘é€æµ‹è¯•è¯·æ±‚
    try:
        print("ğŸ“¡ å‘é€æµ‹è¯•è¯·æ±‚...")
        response = llm.invoke("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
        
        if response and response.content:
            result = f"âœ… æ¨¡å‹è¿æ¥æˆåŠŸï¼\n\n**æ¨¡å‹**: {selected_model}\n**å“åº”**: {str(response.content)[:100]}...\n\nâœ¨ è¯¥æ¨¡å‹å·²å°±ç»ªï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼"
            print(f"âœ… æµ‹è¯•æˆåŠŸ")
        else:
            result = f"âš ï¸ æ¨¡å‹å“åº”å¼‚å¸¸\n\næ¨¡å‹è¿æ¥æˆåŠŸä½†æœªè¿”å›å†…å®¹"
            print("âš ï¸ å“åº”å¼‚å¸¸")
        
        return result
        
    except Exception as e:
        error_msg = str(e)
        result = f"âŒ æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥\n\n**é”™è¯¯ä¿¡æ¯**: {error_msg}\n\n**å¯èƒ½åŸå› **:\n"
        
        if "Connection" in error_msg or "è¿æ¥" in error_msg:
            result += "- ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ\n- éœ€è¦ä»£ç†è®¿é—®ï¼ˆå¦‚ OpenAIï¼‰\n"
        elif "401" in error_msg or "API key" in error_msg:
            result += "- API Key é”™è¯¯æˆ–å·²è¿‡æœŸ\n- è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„é…ç½®\n"
        elif "429" in error_msg or "quota" in error_msg:
            result += "- API é…é¢ä¸è¶³æˆ–è¯·æ±‚è¿‡äºé¢‘ç¹\n- è¯·ç¨åå†è¯•æˆ–å……å€¼\n"
        else:
            result += "- è¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡ºè·å–è¯¦ç»†ä¿¡æ¯\n"
        
        print(f"âŒ æµ‹è¯•å¤±è´¥: {error_msg}")
        return result
    """è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    status = "### ç³»ç»ŸçŠ¶æ€\n\n"
    
    if env_loaded:
        status += "âœ… .env æ–‡ä»¶å·²åŠ è½½\n\n"
    else:
        status += "âš ï¸ æœªæ‰¾åˆ° .env æ–‡ä»¶\n\n"
    
    status += "### API é…ç½®\n\n"
    
    if zhipu_api_key:
        masked = zhipu_api_key[:8] + "***" + zhipu_api_key[-4:] if len(zhipu_api_key) > 12 else "***"
        status += f"- æ™ºè°± AI: {masked}\n"
    else:
        status += "- æ™ºè°± AI: âŒ æœªé…ç½®\n"
    
    if openai_api_key:
        masked = openai_api_key[:8] + "***" + openai_api_key[-4:] if len(openai_api_key) > 12 else "***"
        status += f"- OpenAI: {masked}\n"
    else:
        status += "- OpenAI: âŒ æœªé…ç½®\n"
    
    if groq_api_key:
        masked = groq_api_key[:8] + "***" + groq_api_key[-4:] if len(groq_api_key) > 12 else "***"
        status += f"- Groq: {masked}\n"
    else:
        status += "- Groq: âŒ æœªé…ç½®\n"
    
    if siliconflow_api_key:
        masked = siliconflow_api_key[:8] + "***" + siliconflow_api_key[-4:] if len(siliconflow_api_key) > 12 else "***"
        status += f"- ç¡…åŸºæµåŠ¨: {masked}\n"
    else:
        status += "- ç¡…åŸºæµåŠ¨: âŒ æœªé…ç½®\n"
    
    if current_model_name:
        status += f"\n### å½“å‰æ¨¡å‹\n\nğŸ“Œ {current_model_name}"
    else:
        status += "\n### å½“å‰æ¨¡å‹\n\nâš ï¸ æœªé€‰æ‹©æ¨¡å‹"
    
    return status


# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="LangGraph æ™ºèƒ½ä½“èŠå¤©") as demo:
    gr.Markdown("""
    # ğŸ¤– LangGraph æ™ºèƒ½ä½“èŠå¤©ç•Œé¢
    
    æ¬¢è¿ä½¿ç”¨åŸºäº LangGraph æ„å»ºçš„æ™ºèƒ½ä½“èŠå¤©ç³»ç»Ÿï¼æ”¯æŒå¤šç§å¤§æ¨¡å‹é€‰æ‹©ã€‚
    """)
    
    with gr.Row():
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                height=500,
                label="èŠå¤©çª—å£",
                avatar_images=(None, "ğŸ¤–")
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="åœ¨è¿™é‡Œè¾“å…¥ä½ çš„æ¶ˆæ¯...",
                    scale=4
                )
                submit = gr.Button("å‘é€", variant="primary", scale=1)
            
            with gr.Row():
                clear = gr.Button("æ¸…ç©ºå¯¹è¯")
                retry = gr.Button("é‡è¯•")
        
        with gr.Column(scale=1):
            # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
            available_models = get_available_models()
            default_model = available_models[0] if available_models else None
            
            model_selector = gr.Dropdown(
                choices=[(f"{k} - {AVAILABLE_MODELS[k]['description']}", k) for k in available_models],
                value=default_model,
                label="ğŸ¯ é€‰æ‹©æ¨¡å‹",
                info="é€‰æ‹©æ‚¨æƒ³ä½¿ç”¨çš„å¤§æ¨¡å‹",
                interactive=True
            )
            
            # æ¨¡å‹æµ‹è¯•æŒ‰é’®
            test_btn = gr.Button("ğŸ” æµ‹è¯•å½“å‰æ¨¡å‹è¿æ¥", variant="secondary", size="sm")
            test_result = gr.Markdown("", visible=True)
            
            # åˆå§‹åŒ–é»˜è®¤æ¨¡å‹
            if default_model:
                llm, _ = create_llm(default_model)
                current_llm = llm
                current_model_name = default_model
            
            status_box = gr.Markdown(get_system_status())
            model_info_box = gr.Markdown(get_model_info())
            
            gr.Markdown("""
            ### ğŸ’¡ ä½¿ç”¨æç¤º
            
            1. **é€‰æ‹©æ¨¡å‹** - ä»ä¸Šæ–¹ä¸‹æ‹‰æ¡†é€‰æ‹©
            2. **æµ‹è¯•æ¨¡å‹** - ç‚¹å‡»æµ‹è¯•æŒ‰é’®éªŒè¯è¿æ¥
            3. **å‘é€æ¶ˆæ¯** - è¾“å…¥åç‚¹å‡»å‘é€
            4. **ç‰¹æ®ŠæŒ‡ä»¤**:
               - "ä½ å¥½" â†’ è§¦å‘é—®å€™
               - "å¸®åŠ©" â†’ è·å–å¸®åŠ©ä¿¡æ¯
            
            ### âš™ï¸ é…ç½® API Key
            
            ç¼–è¾‘ `.env` æ–‡ä»¶æ·»åŠ ï¼š
            ```
            SILICONFLOW_API_KEY=ä½ çš„Key
            ZHIPU_API_KEY=ä½ çš„Key
            OPENAI_API_KEY=ä½ çš„Key  
            ```
            
            **å…è´¹æ¨è**:
            - [ç¡…åŸºæµåŠ¨](https://siliconflow.cn/)
            - [æ™ºè°±AI](https://open.bigmodel.cn/)
            """)
    
    # å¤„ç†æ¶ˆæ¯å‘é€
    def user(user_message, history):
        return "", history + [[user_message, None]]
    
    def bot(history, selected_model):
        user_message = history[-1][0]
        bot_message = chat_response(user_message, history, selected_model)
        history[-1][1] = bot_message
        return history
    
    def update_status_wrapper(selected_model):
        """æ›´æ–°çŠ¶æ€æ˜¾ç¤º"""
        return update_status(selected_model)
    
    # ç»‘å®šäº‹ä»¶
    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot, [chatbot, model_selector], chatbot
    )
    submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(
        bot, [chatbot, model_selector], chatbot
    )
    clear.click(lambda: None, None, chatbot, queue=False)
    retry.click(lambda history: history[:-1] if history else history, chatbot, chatbot).then(
        bot, [chatbot, model_selector], chatbot
    )
    model_selector.change(update_status_wrapper, model_selector, status_box)
    test_btn.click(test_model_connection, model_selector, test_result)


if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ å¯åŠ¨ LangGraph æ™ºèƒ½ä½“ç½‘é¡µèŠå¤©ç•Œé¢")
    print("=" * 60)
    print(get_system_status())
    print("\n" + "=" * 60)
    print("âœ¨ æœåŠ¡å¯åŠ¨ä¸­ï¼Œè¯·ç¨å€™...")
    print("=" * 60 + "\n")
    
    demo.launch(
        server_name="127.0.0.1",
        server_port=7861,
        share=False,
        inbrowser=True  # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )
